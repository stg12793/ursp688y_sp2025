{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61688034-24b5-4400-8b06-7e3188b283f1",
   "metadata": {},
   "source": [
    "Urban Data Science & Smart Cities <br>\n",
    "URSP688Y Spring 2025<br>\n",
    "Instructor: Chester Harvey <br>\n",
    "Urban Studies & Planning <br>\n",
    "National Center for Smart Growth <br>\n",
    "University of Maryland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c1002-000c-4080-a9f1-43e319e7d0be",
   "metadata": {},
   "source": [
    "# Exercise03\n",
    "\n",
    "## Problem\n",
    "\n",
    "In the last exercise, you used tabular analyses to examine how eviction impacts tenants in Montgomery and Prince George's Counties. In this exercise, you will use more precise, address-level data to analyze evictions across the entire state through a spatial lens.\n",
    "\n",
    "**You get to write your own research question, but with a few parameters:**\n",
    "- It should be addressable with the provided eviction data\n",
    "- It should require that you relate the eviction data to at least one other dataset, which you provide\n",
    "- It should involve at least one form of spatial analysis (e.g., proximity, overlay, or measurement of another spatial relationship)\n",
    "\n",
    "**Please write a short abstract (200-300 words) at the top of your exercise notebook that concisely summarizes your research question, how you addressed it, and the results of your analysis. Then provide reproducible code in cells below.**\n",
    "\n",
    "### Bonus\n",
    "\n",
    "Only a portion of the eviction records I'm providing for this exercise have addresses that can be geocoded (converting address strings to geographic coordinates) with a high degree of accuracy, or even at all. How could you assess bias in which records are accurately geocoded? (Hint: This will require you to define accuracy.) Can you write a Python script that evaluates whether higher- and lower-accuracy geocodes are randomly distributed across eviction records, or whether certain types of evictions are more or less likely to be geocoded well? **Please report your approach and findings in a separate paragraph and provide supporting code.**\n",
    "\n",
    "## Data\n",
    "\n",
    "[Exercise 3 Google Drive Folder](https://drive.google.com/drive/folders/1-f7CIS-Nw9HWK1f3GUqhTLwhgtzDXVfl?usp=sharing)\n",
    "\n",
    "I'm providing you with eviction warrant data for the whole state of Maryland from 2022 through December 2024. These are from the same District Court of Maryland and Department of Housing and Community Development (DHCD) [source](https://app.powerbigov.us/view?r=eyJrIjoiYWI1Yzg0YjYtNDFkZS00MDUyLThlMDctYmE1ZjY5MGI0MWJhIiwidCI6IjdkM2I4ZDAwLWY5YmUtNDZlNy05NDYwLTRlZjJkOGY3MzE0OSJ9&pageName=ReportSection) as the data from Exercise 2, but also include street addresses. While these data are technically public, it is best practice not to store address-level data on a public GitHub repository. It is also a best practice not to commit large raw data files to Git. For both these reasons, I have shared this dataset in a [Google Drive](https://drive.google.com/drive/folders/1-f7CIS-Nw9HWK1f3GUqhTLwhgtzDXVfl?usp=sharing) folder to which your UMD account has been invited. You should download `md_eviction_warrants_through_2024.csv` store it in the exercise03 directory on your computer before starting to code. \n",
    "\n",
    "There is a `.gitignore` file in the exercise03 directory that prevents any `.csv` file from being tracked by Git. As long as you don't modify this `.gitignore`, the raw data file won't get committed, pushed to your remote fork, or included in a pull request back to the course repo.\n",
    "\n",
    "## File Management and Submitting\n",
    "To submit, please:\n",
    "1. Make a new branch on your fork for this exercise.\n",
    "2. Make a notebook for your exercise with your first name as an underscored suffix (e.g., `exercise02_chester.ipynb`)\n",
    "    - You can either copy this notebook to work off of or start with a fresh notebook. Your choice.\n",
    "4. Make commits to that branch as you work on the exercise.\n",
    "5. Don't commit the eviction warrant CSV or other raw data files to Git.\n",
    "    - Instead, please add any other raw data files your analysis depends on the [Exercise 3 Google Drive Folder](https://drive.google.com/drive/folders/1-f7CIS-Nw9HWK1f3GUqhTLwhgtzDXVfl?usp=sharing).\n",
    "    - The current `.gitignore` will prevent CSV files from committing. Add additional file names/extensions as necessary.\n",
    "6. Make a pull request from your branch. Ensure that the only files included in your pull request are those you intended for this exercise.\n",
    "\n",
    "## Getting Started\n",
    "To get started, here's some code I developed for geocoding the address in each eviction warrant into a geographic coordinate. You can include all or parts of this code in your own exercise, or just run this notebook to produce the `md_eviction_warrants_through_2024.geoparquet` file and import it into your own notebook to use the results.\n",
    "\n",
    "### U.S. Census Geocoder\n",
    "This geocoding process makes use of a [free geocoder provided by the US Census](https://geocoding.geo.census.gov/geocoder/). It's not the most accurate geocoder available, but it's free and fast.\n",
    "\n",
    "### Breaking Code Into Modules\n",
    "In this geocoding process, I'm demonstrating an approach to coding where you break code up into multiple modules and then import names between modules. This helps keep things tidy, allows you to easily reuse code that's generalizable between applications (e.g., the `utils.py` module here), and organize code used for more specific purposes (e.g., the `exercise03.py` and `census_geocode.py` modules).\n",
    "\n",
    "This is exactly how packages work——modules are the basic building blocks. If you wrote an interconnected set of modules to address a certain problem space, you could publish it as a package and let others download it with conda or pip. That's how open-source software gets its start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1473cd2a-1936-41bf-b409-aef05cfa28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import utils\n",
    "import exercise03\n",
    "import census_geocode\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de69f731-f3d2-4cae-a035-498fc20923fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411040"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load warrants and make sure zip codes are stored as strings without decimals\n",
    "warrants_df = pd.read_csv('md_eviction_warrents_through_2024.csv')\n",
    "warrants_df['TenantZipCode'] = warrants_df['TenantZipCode'].astype('Int64').astype('string')\n",
    "len(warrants_df) # How many warrants are we working with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cee92bd-e932-4505-9c0b-16ad6e3c9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411040 warrants input\n",
      "Reduced to 167949 unique addresses\n"
     ]
    }
   ],
   "source": [
    "# Rather than geocoding 400K+ addresses, could we get only the unique ones?\n",
    "geocode_input_df = exercise03.prep_warrants_for_geocoding(warrants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9bf2cf-fd25-43bc-92eb-0f9fecf4bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split dataframe into 17 chunks\n"
     ]
    }
   ],
   "source": [
    "# The Census Geocoder API can only accept up to 10K rows at a time, so we have to break\n",
    "# our dataframe into chunks\n",
    "\n",
    "# Split into dataframes with less than 10K rows each\n",
    "geocode_input_dfs = utils.chunk_dataframe(geocode_input_df, 9999)\n",
    "\n",
    "# Save each dataframe as a CSV without a header\n",
    "utils.save_dfs_to_csv(geocode_input_dfs, 'geocode_inputs', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa32bb8-71de-4561-86aa-032add716e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST MODE: Processing only one file.\n",
      "Processing file: geocode_inputs/df_14.csv\n",
      "Saved results to: geocode_outputs/geocoderesult_df_14.csv\n"
     ]
    }
   ],
   "source": [
    "# Geocode addresses with the Census Geocoder (set test=True to process only one file)\n",
    "census_geocode.geocode_csvs('geocode_inputs', 'geocode_outputs', test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d60a313-0bc1-49d1-afce-230556f176eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recombine outputs from geocoder into a single dataframe\n",
    "geocode_output_df = exercise03.combine_census_geocoded_csvs('geocode_outputs')\n",
    "len(geocode_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f48b44b9-ba81-478e-93cf-50aee3370191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge geocoded address back onto the inputs with separate fields for address, city, state, and zip\n",
    "geocoded_df = geocode_input_df.merge(geocode_output_df, left_index=True, right_index=True)\n",
    "len(geocoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "143884cf-404e-489b-966a-67150bcad99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use address, city, state, and zip columns to join geocodes onto original warrant records\n",
    "warrants_df = warrants_df.merge(geocoded_df, on=['TenantAddress','TenantCity','TenantState','TenantZipCode'])\n",
    "len(warrants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b66070-42a5-4aad-925b-28c392eab33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert warrants into a geodataframe with points\n",
    "warrants_gdf = utils.lonlat_str_to_geodataframe(warrants_df, 'match_lon_lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced85ad1-9998-470a-85ea-2019b614a552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948782424563261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What proportion of records have points?\n",
    "len(warrants_gdf[warrants_gdf.lon.notnull()]) / len(warrants_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d60464a1-7c6e-4bea-8cd1-c5a17ec72b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5974060349391213"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What proportion of records have exact geocode matches?\n",
    "len(warrants_gdf[warrants_gdf.match_type == 'Exact']) / len(warrants_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bf840-c73e-452d-8808-84bf425b7198",
   "metadata": {},
   "source": [
    "Parquet is a file format for tabular data that efficiently stores data of many types, including a 'geoparquet' variant that stores geometries like points, lines, and polygons. You can easily save a dataframe or geodataframe to parquet with the `.to_parquet` method. The resulting file will be much smaller and load faster than more convetional formats, such as CSV or shapefile. The downside is that you won't be able to open it with Excel, ArcGIS, or other conventional desktop software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7fe55db-26e9-4485-b440-9bbd4b9ed444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/miniconda3/envs/688y/lib/python3.12/site-packages (19.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "warrants_gdf.to_parquet('md_eviction_warrants_through_2024.geoparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a4445-885c-45d7-b428-9fdb40c9a9b7",
   "metadata": {},
   "source": [
    "You can load a parquet file back to a dataframe with `pd.read_parquet('filename')` or a geoparquet back to a geodataframe with `gpd.read_parquet('filename')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "404f2115-557b-4016-a807-4485643cd255",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet('md_eviction_warrants_through_2024.geoparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c24e9f62-cecc-4a66-920d-fceabc1d1d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'EventDate',\n",
       " 'EventType',\n",
       " 'EventComment',\n",
       " 'County',\n",
       " 'Location',\n",
       " 'TenantAddress',\n",
       " 'TenantCity',\n",
       " 'TenantState',\n",
       " 'TenantZipCode',\n",
       " 'CaseType',\n",
       " 'CaseNumber',\n",
       " 'EvictedDate',\n",
       " 'Source',\n",
       " 'SourceDate',\n",
       " 'Year',\n",
       " 'EvictionYear',\n",
       " 'unique_id',\n",
       " 'input_address',\n",
       " 'match_status',\n",
       " 'match_type',\n",
       " 'match_address',\n",
       " 'match_lon_lat',\n",
       " 'match_tiger_line_id',\n",
       " 'match_tiger_line_side',\n",
       " 'lon',\n",
       " 'lat',\n",
       " 'geometry']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9517e-e11a-4a98-9597-d24a15344de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
